{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Which linear regression training algorithm can you use if you have a training set with millions of features?\n",
    "\n",
    "Batch GD, Stochastic GD, Mini-batch GD are linear regression training algorithms can be used if I have a training set with millions of features.\n",
    "- Batch GD: This algorithm update model's parameters based on the whole training set. It is possible to save computing times.\n",
    "- Stochastic GD: Each epoch, this algorithm only use 1 instance of the training data to update model's parameter. \n",
    "- Mini-batch GD: This can performance boots from GPUs to compute large matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Suppose the features in your training set have very different scales. Which algorithms might suffer from this, and how? What can you do about it?\n",
    "\n",
    "The gradient descent algorithm might suffer from different scales features. It take more time to reach the minimum. We can use Features Scaling skill to make features have the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Can gradient descent get stuck in a local minimum when training a logistic regression model?\n",
    "\n",
    "Gradient descent can get stuck in a local minimum when training a logistic regression model if you choose learning rate too large or the number of epochs too small. Because Logistic cost function partial derivative looks very much like Linear Regression cost function partial derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Do all gradient descent algorithms lead to the same model, provided you let them run long enough?\n",
    "\n",
    "Maybe no, the figure below shows that the batch GD is actually reaches the minimum, while both Stochastic GD and Mini-batch GD are walk around. You have to let them run long enough and choose the best learning rate, then all GD algorithms will lead to the same model.\n",
    "\n",
    "![gradient_descent_figure](images/gradient_descent_paths_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Suppose you use batch gradient descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "\n",
    "I think the model is overfitting. Perhaps because high learning rate, then the model go over optimal solution and lead to overfitting or number of epochs too large.\n",
    "\n",
    "I will fix this by decreasing learning rate and the number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Is it a good idea to stop mini-batch gradient descent immediately when the validation error goes up?\n",
    "\n",
    "Stop mini-batch gradient descent immediately when the validation error goes up is a good idea. It is mainly because previous validation error could be the minimum. I will copy previous model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Which gradient descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
    "\n",
    "I think mini-batch GD will reach the vicinity of the optimal solution the fastest. Batch GD will actually converge because it use whole training data to compute every epoch. I can make the others converge as well by choosing a good learning schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Suppose you are using polynomial regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "\n",
    "If there is a large gap between the training error and the validation error. Maybe the model is overfitting because the model is perform good on training set but worse on validation.\n",
    "\n",
    "Three ways to solve this:\n",
    "\n",
    "- Increase model's bias to reduce its variance.\n",
    "- Choose right degree of Polynomial Features.\n",
    "- Use Regularized Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Suppose you are using ridge regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter $\\alpha$ or reduce it?\n",
    "\n",
    "That model suffers from high bias because training error and validation error are almost equal and fairly high. I will reduce hyperparameter $\\alpha$ to increase variance and reduce bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Why would you want to use:\n",
    "\n",
    "**a. Ridge regression instead of plain linear regression (i.e., without any\n",
    "regularization)?**\n",
    "\n",
    "**b. Lasso instead of ridge regression?**\n",
    "\n",
    "**c. Elastic net instead of lasso regression?**\n",
    "\n",
    "Answer:\n",
    "\n",
    "**a.** I want to use Ridge regression instead of plain linear regression because it can reduce overfitting situation. Ridge is good for default.\n",
    "\n",
    "**b.** I want to use Lasso instead of Ridge regression because I suspect that a few of features can be useful. Then lasso will perform greater than Ridge.\n",
    "\n",
    "**c.** I use Elastic net instead of Lasso Regression because of flexibility of Elastic. I can modify $r$ ratio to use both of Ridge and Lasso, Lasso will behave erratically when the number of features is greater than the number of training instances or several features are strongly correlated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two logistic regression classifiers or one softmax regression classifier?\n",
    "\n",
    "I will implement two logistic regressions. One for outdoor/indoor, another for day/night, because in this case, the problem is multiple output. Then I use two logistic regressions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
