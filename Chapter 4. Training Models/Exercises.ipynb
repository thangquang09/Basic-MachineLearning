{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Which linear regression training algorithm can you use if you have a training set with millions of features?\n",
    "\n",
    "Batch GD, Stochastic GD, Mini-batch GD are linear regression training algorithms can be used if I have a training set with millions of features.\n",
    "- Batch GD: This algorithm update model's parameters based on the whole training set. It is possible to save computing times.\n",
    "- Stochastic GD: Each epoch, this algorithm only use 1 instance of the training data to update model's parameter. \n",
    "- Mini-batch GD: This can performance boots from GPUs to compute large matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Suppose the features in your training set have very different scales. Which algorithms might suffer from this, and how? What can you do about it?\n",
    "\n",
    "The gradient descent algorithm might suffer from different scales features. It take more time to reach the minimum. We can use Features Scaling skill to make features have the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Can gradient descent get stuck in a local minimum when training a logistic regression model?\n",
    "\n",
    "Gradient descent cannot get stuck in a local minimum when training a logistic regression model. Because Logistic cost function partial derivative looks very much like Linear Regression cost function partial derivative. Both are convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Do all gradient descent algorithms lead to the same model, provided you let them run long enough?\n",
    "\n",
    "Maybe no, the figure below shows that the batch GD is actually reaches the minimum, while both Stochastic GD and Mini-batch GD are walk around. You have to let them run long enough and choose the best learning rate, then all GD algorithms will lead to the same model.\n",
    "\n",
    "![gradient_descent_figure](images/gradient_descent_paths_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Suppose you use batch gradient descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "\n",
    "I think the model is overfitting. Perhaps because high learning rate, then the model go over optimal solution and lead to overfitting or number of epochs too large.\n",
    "\n",
    "I will fix this by decreasing learning rate and the number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Is it a good idea to stop mini-batch gradient descent immediately when the validation error goes up?\n",
    "\n",
    "Stop mini-batch gradient descent immediately when the validation error goes up is can not be a good idea. It is mainly because mini-batch work like stochastic GD. It random then previous solution could not be the minimum. If stop immediately when the validation error goes up maybe you stop too soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Which gradient descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
    "\n",
    "I think stochastic GD will reach the vicinity of the optimal solution the fastest. Batch GD will actually converge because it use whole training data to compute every epoch. I can make the others converge as well by choosing a good learning schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Suppose you are using polynomial regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "\n",
    "If there is a large gap between the training error and the validation error. Maybe the model is overfitting because the model is perform good on training set but worse on validation.\n",
    "\n",
    "Three ways to solve this:\n",
    "\n",
    "- Increase model's bias to reduce its variance.\n",
    "- Choose right degree of Polynomial Features.\n",
    "- Use Regularized Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Suppose you are using ridge regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter $\\alpha$ or reduce it?\n",
    "\n",
    "That model suffers from high bias because training error and validation error are almost equal and fairly high $\\to$ underfitting. I will reduce hyperparameter $\\alpha$ to increase variance and reduce bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Why would you want to use:\n",
    "\n",
    "**a. Ridge regression instead of plain linear regression (i.e., without any\n",
    "regularization)?**\n",
    "\n",
    "**b. Lasso instead of ridge regression?**\n",
    "\n",
    "**c. Elastic net instead of lasso regression?**\n",
    "\n",
    "Answer:\n",
    "\n",
    "**a.** I want to use Ridge regression instead of plain linear regression because it can reduce overfitting situation. Ridge is good for default.\n",
    "\n",
    "**b.** I want to use Lasso instead of Ridge regression because I suspect that a few of features can be useful. Then lasso will perform greater than Ridge.\n",
    "\n",
    "**c.** I use Elastic net instead of Lasso Regression because of flexibility of Elastic. I can modify $r$ ratio to use both of Ridge and Lasso, Lasso will behave erratically when the number of features is greater than the number of training instances or several features are strongly correlated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two logistic regression classifiers or one softmax regression classifier?\n",
    "\n",
    "I will implement two logistic regressions. One for outdoor/indoor, another for day/night, because in this case, the problem is multiple output. Then I use two logistic regressions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Implement batch gradient descent with early stopping for softmax regression without using Scikit-Learn, only NumPy. Use it on a classification task such as the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'target',\n",
       " 'frame',\n",
       " 'target_names',\n",
       " 'DESCR',\n",
       " 'feature_names',\n",
       " 'filename',\n",
       " 'data_module']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(as_frame = True)\n",
    "list(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names, iris.feature_names # 3 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]]\n",
    "y = iris[\"target\"].values\n",
    "\n",
    "X_b = np.c_[np.ones(len(X)), X] # add bias term\n",
    "\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.2\n",
    "total_instances = len(X)\n",
    "test_size = int(total_instances * test_ratio)\n",
    "valid_size = int(total_instances * valid_ratio)\n",
    "train_size = total_instances - test_size - valid_size\n",
    "\n",
    "np.random.seed(42)\n",
    "index_permutation = np.random.permutation(total_instances)\n",
    "\n",
    "X_train = X_b[index_permutation[: train_size]]\n",
    "y_train = y[index_permutation[: train_size]]\n",
    "X_test = X_b[index_permutation[train_size:-test_size]]\n",
    "y_test = y[index_permutation[train_size:-test_size]]\n",
    "X_valid = X_b[index_permutation[-test_size:]]\n",
    "y_valid = y[index_permutation[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 3), (90,))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape ## 4 features, 90 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some equations\n",
    "\n",
    "**1. Softmax score for class $k$**\n",
    "\n",
    "$$s_k(\\mathbf{x})=\\left(\\mathbf{\\theta}^{(k)}\\right)^\\intercal\\mathbf{x}$$\n",
    "\n",
    "**2. Softmax function**\n",
    "\n",
    "$$\\widehat{p}_k=\\sigma(\\mathbf{s}(\\mathbf{x}))_k=\\frac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\Sigma_{j = 1}^K \\exp\\left(s_j(\\mathbf{x})\\right)}$$\n",
    "\n",
    "**3. Softmax regression classifier prediction**\n",
    "\n",
    "$$\\widehat{y}=\\underset{k}{\\operatorname*{argmax}} \\sigma(\\mathbf{s}(\\mathbf{x}))_k=\\underset{k}{\\operatorname*{argmax}} s_k(\\mathbf{x})=\\underset{k}{\\operatorname*{argmax}} \\left(\\left(\\mathbf{\\theta}^{(k)}\\right)^{\\mathsf{T}}\\mathbf{x}\\right)$$\n",
    "\n",
    "**4. Coss entropy cost function**\n",
    "\n",
    "$$J(\\boldsymbol{\\Theta})=-\\frac{1}{m}\\Sigma_{i = 1}^m\\Sigma_{k = 1}^Ky_k^{(i)}\\mathrm{log}\\Big(\\widehat{p}_k^{(i)}\\Big)$$\n",
    "\n",
    "$$\\frac\\partial{\\partial\\theta_j}\\mathrm{J}(\\mathbf{\\theta})=\\frac1m\\sum_{i = 1}^m\\left(\\sigma\\Big(\\mathbf{\\theta}^\\mathsf{T}\\mathbf{x}^{(i)}\\Big)-y^{(i)}\\Big)x_j^{(i)}\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale data\n",
    "mean = X_train[:, 1:].mean(axis = 0)\n",
    "std = X_train[:, 1:].std(axis = 0)\n",
    "X_train[:, 1:] = (X_train[:, 1:] - mean) / std\n",
    "X_test[:, 1:] = (X_test[:, 1:]- mean) / std\n",
    "X_valid[:, 1:] = (X_valid[:, 1:] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    ones = np.zeros((len(y), y.max() + 1))\n",
    "    for i in range(len(y)):\n",
    "        ones[i][y[i]] = 1\n",
    "    return ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = one_hot(y_train)\n",
    "y_valid = one_hot(y_valid)\n",
    "y_test = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = X_b.shape[1] \n",
    "K = len(np.unique(y))\n",
    "n_features, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_function(logits):\n",
    "    return np.exp(logits) / np.exp(logits).sum(axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "n_epochs = 5000\n",
    "m = len(X_train)\n",
    "epsilon = 1e-5\n",
    "\n",
    "np.random.seed(42)\n",
    "Theta = np.random.rand(n_features, K)\n",
    "\n",
    "entropys = []\n",
    "best_Theta = 10e7\n",
    "best_epoch = 10e7\n",
    "best_entropy_loss = 10e7\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    logits = X_train @ Theta\n",
    "    p_hat_train = softmax_function(logits)\n",
    "    p_hat_valid = softmax_function(X_valid @ Theta)\n",
    "    error_train = p_hat_train - y_train\n",
    "    error_valid = p_hat_valid - y_valid\n",
    "\n",
    "    entropy_loss = (-y_valid * np.log(p_hat_valid + epsilon)).sum(axis = 1).mean()\n",
    "    if len(entropys) > 1 and entropy_loss > entropys[-1]:\n",
    "        if best_entropy_loss and best_entropy_loss > entropys[-1]:\n",
    "            best_entropy_loss = entropys[-1]\n",
    "            best_Theta = Theta\n",
    "            best_epoch = epoch\n",
    "\n",
    "    \n",
    "    entropys.append(entropy_loss)\n",
    "    gradients = 1/m * X_train.T @ error_train\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best entropy loss = 0.07125980803117586, best_epoch = 2150\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best entropy loss = {best_entropy_loss}, best_epoch = {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51666075,  5.32987885, -3.78929124],\n",
       "       [-5.11371527,  0.19839672,  5.8259902 ],\n",
       "       [-4.68005491, -0.24497502,  6.45040469]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.linspace(1, n_epochs, n_epochs), entropys, \"b-\")\n",
    "plt.plot([best_epoch], [best_entropy_loss], \"r.\", markersize = 10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = X_valid @ best_Theta\n",
    "p_hat = softmax_function(logits)\n",
    "y_predict_valid = p_hat.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_valid = (one_hot(y_predict_valid) == y_valid).mean()\n",
    "accuracy_valid.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_test = X_test @ best_Theta\n",
    "p_hat_test = softmax_function(logits_test)\n",
    "y_predict_test = p_hat_test.argmax(axis = 1)\n",
    "accuracy_test = (one_hot(y_predict_test) == y_test).mean()\n",
    "accuracy_test.round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
